<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ingenuity - Transformers</title>
    <meta name="description" content="Ingenuity - AI Language Model Interface">
    <link rel="stylesheet" href="styles.css">
    <link rel="icon" href="https://media.licdn.com/dms/image/v2/D4D0BAQGRXrchbVLnmQ/company-logo_200_200/B4DZcfVDG5GkAI-/0/1748577301437/ingenuity_amen_logo?e=1753920000&amp;v=beta&amp;t=t6AbqrmNNxzIwNRxMNfZiqUFGvyU6oMxHtAwHTH-WPM" type="image/png">
    <style>
        .content-page .main-content {
            justify-content: flex-start;
            align-items: flex-start;
            padding-top: 60px;
        }
        
        .page-title {
            font-size: 48px;
            font-weight: 700;
            color: #111111;
            margin-bottom: 40px;
            line-height: 1.2;
        }
        
        .page-content {
            max-width: 600px;
            font-size: 18px;
            line-height: 1.6;
            color: #333333;
        }
        
        .page-content h2 {
            font-size: 32px;
            font-weight: 600;
            color: #111111;
            margin: 40px 0 20px 0;
            line-height: 1.3;
        }
        
        .page-content p {
            margin-bottom: 20px;
        }
        
        .content-area .info-section {
            background: #ffffff;
            border-radius: 8px;
            padding: 20px;
            border: 1px solid #e0e0e0;
            margin-bottom: 20px;
        }
        
        .info-section h3 {
            font-size: 14px;
            font-weight: 600;
            color: #666666;
            margin-bottom: 8px;
            text-transform: uppercase;
            letter-spacing: 0.5px;
        }
        
        .info-section p {
            font-size: 12px;
            color: #888888;
            line-height: 1.4;
            margin: 0;
        }
    </style>
</head>

<body class="content-page">
    <header>
        <h1>Ingenuity</h1>
    </header>

    <div class="layout-container">
        <!-- Left Sidebar - Navigation -->
        <aside class="sidebar">
            <nav class="sidebar-nav">
                <a href="index.html" class="nav-item">Home</a>
                <a href="transformers.html" class="nav-item">Transformers</a>
                <a href="advanced-ai.html" class="nav-item">Advanced AI</a>
                <a href="qwen3-235b.html" class="nav-item">Qwen 3.5B</a>
                <a href="gemini.html" class="nav-item">Gemini</a>
                <a href="documentation.html" class="nav-item">Documentation</a>
                <a href="ai-ethics.html" class="nav-item">AI Ethics</a>
                <a href="about.html" class="nav-item">About</a>
                <a href="settings.html" class="nav-item">Settings</a>
            </nav>
        </aside>

        <!-- Main Content Area -->
        <main class="main-content">
            <h1 class="page-title">Understanding Transformers</h1>
            
            <div class="page-content">
                <h2>What are Transformers?</h2>
                <p>Transformers are a revolutionary neural network architecture that has transformed the field of natural language processing and artificial intelligence. Introduced in the paper "Attention Is All You Need" by Vaswani et al., transformers have become the foundation for most modern AI language models.</p>
                
                <h2>Key Innovation</h2>
                <p>The transformer architecture introduced the concept of self-attention, allowing models to weigh the importance of different words in a sequence when processing each word. This mechanism enables the model to capture long-range dependencies and contextual relationships more effectively than previous architectures.</p>

                <h2>How They Work</h2>
                <p>Transformers process sequences of data through multiple layers of attention mechanisms and feed-forward networks. Each layer allows the model to build increasingly complex representations of the input, enabling sophisticated understanding and generation of human language.</p>
                
                <p>The attention mechanism allows the model to focus on relevant parts of the input when generating each part of the output, making it particularly effective for tasks like translation, summarization, and question answering.</p>
            </div>
        </main>

        <!-- Right Content Area -->
        <section class="content-area">
            <div class="info-section">
                <h3>Architecture</h3>
                <p>Self-attention mechanism<br>Multi-layer processing</p>
            </div>
            
            <div class="info-section">
                <h3>Applications</h3>
                <p>Language models<br>Translation systems</p>
            </div>
            
            <div class="info-section">
                <h3>Impact</h3>
                <p>Revolutionary AI advancement<br>Foundation of modern NLP</p>
            </div>
        </section>
    </div>

    <script src="security.js"></script>
    <script src="script.js"></script>
</body>
</html>